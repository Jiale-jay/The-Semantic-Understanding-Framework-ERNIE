{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from utils.data_processor import convert_example, create_dataloader\n",
    "\n",
    "import paddle\n",
    "import paddlenlp\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "\n",
    "\n",
    "glue_tasks = [\"cola\", \"sst-2\", \"sts-b\", \"qqp\", \"mnli\", \"qnli\", \"rte\", \"wnli\", \"mrpc\"]\n",
    "\n",
    "glue_tasks_num_labels = {\n",
    "    \"cola\": 2,\n",
    "    \"sst-2\": 2,\n",
    "    \"sts-b\": 1,\n",
    "    \"qqp\": 2,\n",
    "    \"mnli\": 3,\n",
    "    \"qnli\": 2,\n",
    "    \"rte\": 2,\n",
    "    \"wnli\": 3,\n",
    "    \"mrpc\": 2\n",
    "}\n",
    "\n",
    "glue_task_type = {\n",
    "    \"cola\": \"classification\",\n",
    "    \"sst-2\": \"classification\",\n",
    "    \"sts-b\": \"regression\" ,\n",
    "    \"qqp\": \"classification\",\n",
    "    \"mnli\": \"classification\",\n",
    "    \"qnli\": \"classification\",\n",
    "    \"rte\": \"classification\",\n",
    "    \"wnli\": \"classification\",\n",
    "    \"mrpc\": \"classification\"\n",
    "}\n",
    "\n",
    "def load_glue_sub_data(name):\n",
    "    if name not in glue_tasks:\n",
    "        raise Exception(\"Name Error: name must in \", glue_tasks)\n",
    "    \n",
    "    splits = (\"train\", \"dev\")\n",
    "    \n",
    "    if name == \"mnli\":\n",
    "        splits = (\"train\", \"dev_matched\")\n",
    "    \n",
    "    dataset = load_dataset(\"glue\", name=name, splits=splits)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# task_name = \"wnli\"\n",
    "# train, dev, test = load_glue_sub_data(task_name)\n",
    "# print(test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErnieForSequenceClassification(paddle.nn.Layer):\n",
    "    \"\"\"\n",
    "    Model for sentence (pair) classification task with ERNIE.\n",
    "    \"\"\"\n",
    "    def __init__(self, ernie, num_class=2, dropout=None):\n",
    "        super(ErnieForSequenceClassification, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.ernie = ernie\n",
    "        self.dropout = paddle.nn.Dropout(dropout if dropout is not None else self.ernie.config[\"hidden_dropout_prob\"])\n",
    "        self.classifier = paddle.nn.Linear(self.ernie.config[\"hidden_size\"], num_class)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "        _, pooled_output = self.ernie(input_ids, token_type_ids=token_type_ids, position_ids=position_ids, attention_mask=attention_mask)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"sets random seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    paddle.seed(seed)\n",
    "\n",
    "# 定义任务参数\n",
    "task_name = \"cola\"\n",
    "model_name = \"ernie-2.0-en\"\n",
    "\n",
    "# 定义超参数\n",
    "epochs={\n",
    "    \"cola\":3,\n",
    "    \"sst-2\":4,\n",
    "    \"sts-b\":3,\n",
    "    \"qqp\":3,\n",
    "    \"mnli\":3,\n",
    "    \"qnli\":4,\n",
    "    \"rte\":4,\n",
    "    \"wnli\":4,\n",
    "    \"mrpc\":4\n",
    "\n",
    "}\n",
    "\n",
    "learning_rate={\n",
    "    \"cola\":3e-5,\n",
    "    \"sst-2\":2e-5,\n",
    "    \"sts-b\":5e-5,\n",
    "    \"qqp\":3e-5,\n",
    "    \"mnli\":3e-5,\n",
    "    \"qnli\":2e-5,\n",
    "    \"rte\":2e-5,\n",
    "    \"wnli\":2e-5,\n",
    "    \"mrpc\":3e-5\n",
    "}\n",
    "\n",
    "batch_size=32\n",
    "warmup_proportion = 0.1\n",
    "weight_decay = 0.01\n",
    "max_seq_length= 128\n",
    "\n",
    "\n",
    "# 设置环境\n",
    "set_seed(0)\n",
    "paddle.set_device(\"gpu:0\")\n",
    "\n",
    "# 加载和处理数据\n",
    "train_ds, dev_ds  = load_glue_sub_data(task_name)\n",
    "\n",
    "tokenizer = paddlenlp.transformers.ErnieTokenizer.from_pretrained(model_name)\n",
    "trans_func = partial(convert_example, task_name=task_name, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
    "batchify_fn =  lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id), # input\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id), # segment\n",
    "    Stack(dtype=\"int64\") if glue_task_type[task_name]==\"classification\" else Stack(dtype=\"float32\") # label\n",
    "): [data for data in fn(samples)]\n",
    "\n",
    "train_data_loader = create_dataloader(train_ds, mode=\"train\", batch_size=batch_size[task_name], batchify_fn=batchify_fn, trans_fn=trans_func)\n",
    "dev_data_loader = create_dataloader(dev_ds, mode=\"dev\", batch_size=batch_size[task_name], batchify_fn=batchify_fn, trans_fn=trans_func)\n",
    "\n",
    "# 模型实例化\n",
    "ernie_model = paddlenlp.transformers.ErnieModel.from_pretrained(model_name)\n",
    "model = ErnieForSequenceClassification(ernie_model, num_class=glue_tasks_num_labels[task_name])\n",
    "\n",
    "# 设置lr_scheduler\n",
    "num_training_steps = len(train_data_loader) * epochs[task_name]\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate[task_name], num_training_steps, warmup_proportion)\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100%|██████████| 369/369 [00:00<00:00, 5040.16it/s]\n",
    "[2021-08-19 11:27:41,116] [    INFO] - Downloading vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/ernie_v2_base/vocab.txt\n",
    "100%|██████████| 227/227 [00:00<00:00, 4079.07it/s]\n",
    "[2021-08-19 11:27:41,265] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/ernie_v2_base/ernie_v2_eng_base.pdparams and saved to /home/aistudio/.paddlenlp/models/ernie-2.0-en\n",
    "[2021-08-19 11:27:41,267] [    INFO] - Downloading ernie_v2_eng_base.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/ernie_v2_base/ernie_v2_eng_base.pdparams\n",
    "100%|██████████| 427692/427692 [00:06<00:00, 68215.99it/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle.nn.functional as F\n",
    "from utils.metrics import compute_metrics\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, data_loader, task_type=\"classification\"):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    preds = None\n",
    "    out_labels = None\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        if task_type == \"classification\":\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            losses.append(loss.numpy())\n",
    "\n",
    "            if preds is None:\n",
    "                preds = np.argmax(logits.detach().numpy(), axis=1).reshape([len(logits), 1])\n",
    "                out_labels = labels.detach().numpy()\n",
    "            else:\n",
    "                preds = np.append(preds, np.argmax(logits.detach().numpy(), axis=1).reshape([len(logits), 1]), axis=0)\n",
    "                out_labels = np.append(out_labels, labels.detach().numpy(), axis=0)\n",
    "        else:\n",
    "            loss = F.mse_loss(logits, labels)\n",
    "            losses.append(loss.numpy())\n",
    "\n",
    "            if preds is None:\n",
    "                preds = logits.detach().numpy()\n",
    "                out_labels = labels.detach().numpy()\n",
    "            else:\n",
    "                preds = np.append(preds, logits.detach().numpy(), axis=0)\n",
    "                out_labels = np.append(out_labels, labels.detach().numpy(), axis=0)\n",
    "    \n",
    "    result = compute_metrics(task_name, preds.reshape(-1), out_labels.reshape(-1))\n",
    "    print(\"evaluate result: \",result)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def do_train():\n",
    "    model.train()\n",
    "    for  epoch in range(1, epochs[task_name]+1):\n",
    "        for step, batch in enumerate(train_data_loader, start=1):\n",
    "            input_ids, segment_ids, labels = batch\n",
    "            logits = model(input_ids, segment_ids)\n",
    "            if glue_task_type[task_name] == \"classification\":\n",
    "                loss = F.cross_entropy(logits, labels)\n",
    "            else:\n",
    "                loss = F.mse_loss(logits, labels)\n",
    "\n",
    "            if step%20 == 0:\n",
    "                print(\"epoch: {}/{}, step: {}/{}, loss: {} \".format(epoch, epochs[task_name], step, len(train_data_loader), loss.numpy()))\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "        \n",
    "        evaluate(model, dev_data_loader, task_type=glue_task_type[task_name])\n",
    "\n",
    "# 开始模型训练\n",
    "do_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch: 1/3, step: 20/268, loss: [0.13451196] \n",
    "epoch: 1/3, step: 40/268, loss: [0.06055285] \n",
    "epoch: 1/3, step: 60/268, loss: [0.02542446] \n",
    "epoch: 1/3, step: 80/268, loss: [0.1956732] \n",
    "epoch: 1/3, step: 100/268, loss: [0.0759825] \n",
    "epoch: 1/3, step: 120/268, loss: [0.11043643] \n",
    "epoch: 1/3, step: 140/268, loss: [0.0810241] \n",
    "epoch: 1/3, step: 160/268, loss: [0.09277922] \n",
    "epoch: 1/3, step: 180/268, loss: [0.02443065] \n",
    "epoch: 1/3, step: 200/268, loss: [0.12841964] \n",
    "epoch: 1/3, step: 220/268, loss: [0.1023003] \n",
    "epoch: 1/3, step: 240/268, loss: [0.05106329] \n",
    "epoch: 1/3, step: 260/268, loss: [0.11647318] \n",
    "evaluate result:  {'mcc': 0.6244666931728993}\n",
    "epoch: 2/3, step: 20/268, loss: [0.2604182] \n",
    "epoch: 2/3, step: 40/268, loss: [0.03504209] \n",
    "epoch: 2/3, step: 60/268, loss: [0.15183952] \n",
    "epoch: 2/3, step: 80/268, loss: [0.11418241] \n",
    "epoch: 2/3, step: 100/268, loss: [0.12136559] \n",
    "epoch: 2/3, step: 120/268, loss: [0.04706593] \n",
    "epoch: 2/3, step: 140/268, loss: [0.01597433] \n",
    "epoch: 2/3, step: 160/268, loss: [0.12209365] \n",
    "epoch: 2/3, step: 180/268, loss: [0.04617871] \n",
    "epoch: 2/3, step: 200/268, loss: [0.01696031] \n",
    "epoch: 2/3, step: 220/268, loss: [0.10801802] \n",
    "epoch: 2/3, step: 240/268, loss: [0.02620783] \n",
    "epoch: 2/3, step: 260/268, loss: [0.11894499] \n",
    "evaluate result:  {'mcc': 0.6244666931728993}\n",
    "epoch: 3/3, step: 20/268, loss: [0.02899554] \n",
    "epoch: 3/3, step: 40/268, loss: [0.09603783] \n",
    "epoch: 3/3, step: 60/268, loss: [0.1549092] \n",
    "epoch: 3/3, step: 80/268, loss: [0.11537941] \n",
    "epoch: 3/3, step: 100/268, loss: [0.06026918] \n",
    "epoch: 3/3, step: 120/268, loss: [0.06986095] \n",
    "epoch: 3/3, step: 140/268, loss: [0.06762329] \n",
    "epoch: 3/3, step: 160/268, loss: [0.11107697] \n",
    "epoch: 3/3, step: 180/268, loss: [0.13259125] \n",
    "epoch: 3/3, step: 200/268, loss: [0.32414383] \n",
    "epoch: 3/3, step: 220/268, loss: [0.01820294] \n",
    "epoch: 3/3, step: 240/268, loss: [0.08801244] \n",
    "epoch: 3/3, step: 260/268, loss: [0.27648443] \n",
    "evaluate result:  {'mcc': 0.6244666931728993}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
