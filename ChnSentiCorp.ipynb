{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./ERNIE')\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import paddle as P\n",
    "\n",
    "from ernie.tokenizing_ernie import ErnieTokenizer\n",
    "from ernie.modeling_ernie import ErnieModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置好所有的超参数，对于ERNIE任务学习率推荐取1e-5/2e-5/5e-5, 根据显存大小调节BATCH大小, 最大句子长度不超过512.\n",
    "BATCH=32\n",
    "MAX_SEQLEN=300\n",
    "LR=5e-5\n",
    "EPOCH=10\n",
    "\n",
    "\n",
    "ernie = ErnieModelForSequenceClassification.from_pretrained('ernie-1.0', num_labels=3)\n",
    "optimizer = P.optimizer.Adam(LR,parameters=ernie.parameters())\n",
    "tokenizer = ErnieTokenizer.from_pretrained('ernie-1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[INFO] 2021-11-12 17:41:25,477 [modeling_ernie.py:  267]:    get pretrain dir from https://ernie-github.cdn.bcebos.com/model-ernie1.0.1.tar.gz\n",
    "INFO:filelock:Lock 140073509841552 acquired on /home/aistudio/.paddle-ernie-cache/466eabcffd6d6a83ae9cb97dd1a167bd.lock\n",
    "INFO:filelock:Lock 140073509841552 released on /home/aistudio/.paddle-ernie-cache/466eabcffd6d6a83ae9cb97dd1a167bd.lock\n",
    "W1112 17:41:25.554248   173 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
    "W1112 17:41:25.559780   173 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n",
    "[INFO] 2021-11-12 17:41:34,646 [modeling_ernie.py:  285]:    loading pretrained model from /home/aistudio/.paddle-ernie-cache/466eabcffd6d6a83ae9cb97dd1a167bd\n",
    "./ERNIE/ernie/modeling_ernie.py:296: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
    "  log.warn('param:%s not set in pretrained model, skip' % k)\n",
    "[WARNING] 2021-11-12 17:41:38,150 [modeling_ernie.py:  296]:    param:classifier.weight not set in pretrained model, skip\n",
    "[WARNING] 2021-11-12 17:41:38,151 [modeling_ernie.py:  296]:    param:classifier.bias not set in pretrained model, skip\n",
    "[INFO] 2021-11-12 17:41:38,601 [tokenizing_ernie.py:  102]:    get pretrain dir from https://ernie-github.cdn.bcebos.com/model-ernie1.0.1.tar.gz\n",
    "INFO:filelock:Lock 140073507419280 acquired on /home/aistudio/.paddle-ernie-cache/466eabcffd6d6a83ae9cb97dd1a167bd.lock\n",
    "INFO:filelock:Lock 140073507419280 released on /home/aistudio/.paddle-ernie-cache/466eabcffd6d6a83ae9cb97dd1a167bd.lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(path):\n",
    "    data = []\n",
    "    for i, l in enumerate(open(path)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        l = l.strip().split('\\t')\n",
    "        text, label = l[0], int(l[1])\n",
    "        text_id, _ = tokenizer.encode(text) # ErnieTokenizer 会自动添加ERNIE所需要的特殊token，如[CLS], [SEP]\n",
    "        text_id = text_id[:MAX_SEQLEN]\n",
    "        text_id = np.pad(text_id, [0, MAX_SEQLEN-len(text_id)], mode='constant') # 对所有句子都补长至300，这样会比较费显存；\n",
    "        label_id = np.array(label+1)\n",
    "        data.append((text_id, label_id))\n",
    "    return data\n",
    "\n",
    "train_data = make_data('./chnsenticorp/train/part.0')\n",
    "test_data = make_data('./chnsenticorp/dev/part.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_data(data, i):\n",
    "    d = data[i*BATCH: (i + 1) * BATCH]\n",
    "    feature, label = zip(*d)\n",
    "    feature = np.stack(feature)  # 将BATCH行样本整合在一个numpy.array中\n",
    "    label = np.stack(list(label))\n",
    "    feature = P.to_tensor(feature) # 使用to_variable将numpy.array转换为paddle tensor\n",
    "    label = P.to_tensor(label)\n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(EPOCH):\n",
    "    np.random.shuffle(train_data) # 每个epoch都shuffle数据以获得最佳训练效果；\n",
    "    #train\n",
    "    for j in range(len(train_data) // BATCH):\n",
    "        feature, label = get_batch_data(train_data, j)\n",
    "        loss, _ = ernie(feature, labels=label) # ernie模型的返回值包含(loss, logits)；其中logits目前暂时不需要使用\n",
    "        loss.backward()\n",
    "        optimizer.minimize(loss)\n",
    "        ernie.clear_gradients()\n",
    "        if j % 10 == 0:\n",
    "            print('train %d: loss %.5f' % (j, loss.numpy()))\n",
    "        # evaluate\n",
    "        if j % 100 == 0:\n",
    "            all_pred, all_label = [], []\n",
    "            with P.no_grad(): # 在这个with域内ernie不会进行梯度计算；\n",
    "                ernie.eval() # 控制模型进入eval模式，这将会关闭所有的dropout；\n",
    "                for j in range(len(test_data) // BATCH):\n",
    "                    feature, label = get_batch_data(test_data, j)\n",
    "                    loss, logits = ernie(feature, labels=label) \n",
    "                    all_pred.extend(logits.argmax(-1).numpy())\n",
    "                    all_label.extend(label.numpy())\n",
    "                ernie.train()\n",
    "            f1 = f1_score(all_label, all_pred, average='macro')\n",
    "            acc = (np.array(all_label) == np.array(all_pred)).astype(np.float32).mean()\n",
    "            print('acc %.5f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train 0: loss 0.86285\n",
    "acc 0.50084\n",
    "train 10: loss 0.66456\n",
    "train 20: loss 0.46766\n",
    "train 30: loss 0.31338\n",
    "train 40: loss 0.52718\n",
    "train 50: loss 0.28027\n",
    "train 60: loss 0.18098\n",
    "train 70: loss 0.38587\n",
    "train 80: loss 0.25202\n",
    "train 90: loss 0.10612\n",
    "train 100: loss 0.36271\n",
    "acc 0.91216\n",
    "train 110: loss 0.19755\n",
    "train 120: loss 0.15493\n",
    "train 130: loss 0.18104\n",
    "train 140: loss 0.27483\n",
    "train 150: loss 0.06898\n",
    "train 160: loss 0.18187\n",
    "train 170: loss 0.24634\n",
    "train 180: loss 0.11194\n",
    "train 190: loss 0.31004\n",
    "train 200: loss 0.18383\n",
    "acc 0.92990\n",
    "train 210: loss 0.39319\n",
    "train 220: loss 0.17911\n",
    "train 230: loss 0.15745\n",
    "train 240: loss 0.40289\n",
    "train 250: loss 0.18617\n",
    "train 260: loss 0.36464\n",
    "train 270: loss 0.24336\n",
    "train 280: loss 0.05639\n",
    "train 290: loss 0.29951\n",
    "train 0: loss 0.10927\n",
    "acc 0.93074\n",
    "train 10: loss 0.15014\n",
    "train 20: loss 0.07078\n",
    "train 30: loss 0.02730\n",
    "train 40: loss 0.04488\n",
    "train 50: loss 0.03658\n",
    "train 60: loss 0.14192\n",
    "train 70: loss 0.36672\n",
    "train 80: loss 0.15562\n",
    "train 90: loss 0.14741\n",
    "train 100: loss 0.07760\n",
    "acc 0.90541\n",
    "train 110: loss 0.16851\n",
    "train 120: loss 0.08833\n",
    "train 130: loss 0.10430\n",
    "train 140: loss 0.04092\n",
    "train 150: loss 0.11011\n",
    "train 160: loss 0.12601\n",
    "train 170: loss 0.01970\n",
    "train 180: loss 0.02083\n",
    "train 190: loss 0.02704\n",
    "train 200: loss 0.04330\n",
    "acc 0.94172\n",
    "train 210: loss 0.05111\n",
    "train 220: loss 0.04243\n",
    "train 230: loss 0.01082\n",
    "train 240: loss 0.09999\n",
    "train 250: loss 0.10365\n",
    "train 260: loss 0.05661\n",
    "train 270: loss 0.12513\n",
    "train 280: loss 0.24553\n",
    "train 290: loss 0.01133\n",
    "train 0: loss 0.19247\n",
    "acc 0.94679\n",
    "train 10: loss 0.05103\n",
    "train 20: loss 0.07351\n",
    "train 30: loss 0.15348\n",
    "train 40: loss 0.00540\n",
    "train 50: loss 0.02154\n",
    "train 60: loss 0.19600\n",
    "train 70: loss 0.01916\n",
    "train 80: loss 0.06111\n",
    "train 90: loss 0.07352\n",
    "train 100: loss 0.06478\n",
    "acc 0.94426\n",
    "train 110: loss 0.03744\n",
    "train 120: loss 0.02267\n",
    "train 130: loss 0.00507\n",
    "train 140: loss 0.17992\n",
    "train 150: loss 0.10633\n",
    "train 160: loss 0.04254\n",
    "train 170: loss 0.04541\n",
    "train 180: loss 0.03623\n",
    "train 190: loss 0.02851\n",
    "train 200: loss 0.07782\n",
    "acc 0.93666\n",
    "train 210: loss 0.04136\n",
    "train 220: loss 0.17985\n",
    "train 230: loss 0.04763\n",
    "train 240: loss 0.11269\n",
    "train 250: loss 0.04453\n",
    "train 260: loss 0.15736\n",
    "train 270: loss 0.35397\n",
    "train 280: loss 0.07969\n",
    "train 290: loss 0.04847\n",
    "train 0: loss 0.33155\n",
    "acc 0.85642\n",
    "train 10: loss 0.17980\n",
    "train 20: loss 0.24665\n",
    "train 30: loss 0.16764\n",
    "train 40: loss 0.03118\n",
    "train 50: loss 0.05369\n",
    "train 60: loss 0.06388\n",
    "train 70: loss 0.04429\n",
    "train 80: loss 0.02393\n",
    "train 90: loss 0.04675\n",
    "train 100: loss 0.09336\n",
    "acc 0.93412\n",
    "train 110: loss 0.02698\n",
    "train 120: loss 0.07860\n",
    "train 130: loss 0.03140\n",
    "train 140: loss 0.01014\n",
    "train 150: loss 0.04252\n",
    "train 160: loss 0.05134\n",
    "train 170: loss 0.20238\n",
    "train 180: loss 0.00942\n",
    "train 190: loss 0.12322\n",
    "train 200: loss 0.31296\n",
    "acc 0.93497\n",
    "train 210: loss 0.28249\n",
    "train 220: loss 0.04357\n",
    "train 230: loss 0.02566\n",
    "train 240: loss 0.01707\n",
    "train 250: loss 0.23882\n",
    "train 260: loss 0.00347\n",
    "train 270: loss 0.04164\n",
    "train 280: loss 0.22989\n",
    "train 290: loss 0.09394\n",
    "train 0: loss 0.01540\n",
    "acc 0.94764\n",
    "train 10: loss 0.15776\n",
    "train 20: loss 0.01940\n",
    "train 30: loss 0.00642\n",
    "train 40: loss 0.15567\n",
    "train 50: loss 0.00355\n",
    "train 60: loss 0.00412\n",
    "train 70: loss 0.01525\n",
    "train 80: loss 0.17096\n",
    "train 90: loss 0.00169\n",
    "train 100: loss 0.04087\n",
    "acc 0.94510\n",
    "train 110: loss 0.02118\n",
    "train 120: loss 0.02053\n",
    "train 130: loss 0.39884\n",
    "train 140: loss 0.05539\n",
    "train 150: loss 0.01432\n",
    "train 160: loss 0.00613\n",
    "train 170: loss 0.05895\n",
    "train 180: loss 0.03971\n",
    "train 190: loss 0.00688\n",
    "train 200: loss 0.02204\n",
    "acc 0.93834\n",
    "train 210: loss 0.00522\n",
    "train 220: loss 0.00888\n",
    "train 230: loss 0.02209\n",
    "train 240: loss 0.01645"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
